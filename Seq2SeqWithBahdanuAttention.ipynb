{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d166da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49dc2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde9f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d93b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01de3bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18cf04e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jesan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jesan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jesan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jesan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7477c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 0\n",
    "sos = 1\n",
    "eos = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9146da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135842\n"
     ]
    }
   ],
   "source": [
    "lines = open('eng-fra.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea6b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1acfc99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135842\n",
      "135842\n"
     ]
    }
   ],
   "source": [
    "x = ([])\n",
    "y = ([])\n",
    "\n",
    "for l in lines:\n",
    "    temp1, temp2 = [], []\n",
    "    s1, s2 = l.split('\\t')\n",
    "    temp1.append(normalizeString(s1))\n",
    "    temp2.append(normalizeString(s2))\n",
    "    x.append(temp1)\n",
    "    y.append(temp2)\n",
    "\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "\n",
    "#for i in range(0, 5):\n",
    "    #print(x[i])\n",
    "    #print(y[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4c54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotdic(data, language):\n",
    "    vocab = []\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    for i in range(0, len(data)):\n",
    "        sent = data[i]\n",
    "        temp = ''\n",
    "        temp = ''.join(sent)\n",
    "        for word in temp.split():\n",
    "            if word not in stop_words:\n",
    "                vocab.append(word)\n",
    "    corpus = Counter(vocab)\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)\n",
    "\n",
    "    onehot_dic = {'PAD': 0, 'SOS': 1, 'EOS': 2}\n",
    "\n",
    "    for i,w in enumerate(corpus_):\n",
    "        onehot_dic[w] = i\n",
    "\n",
    "    return onehot_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4670c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "engonehot_dic = onehotdic(x, 'english')\n",
    "fronehot_dic = onehotdic(y, 'french')\n",
    "#print(engonehot_dic)\n",
    "#print(fronehot_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6408d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenToindex(data, onehot_dict):\n",
    "    final = []\n",
    "    for i in range(0, len(data)):\n",
    "        sent = data[i]\n",
    "        temp = ''\n",
    "        temp = ''.join(sent)\n",
    "        final.append([onehot_dict[word] for word in temp.split()\n",
    "                             if word in onehot_dict.keys()])\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d4aaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[280, 33]\n",
      "[280, 33]\n"
     ]
    }
   ],
   "source": [
    "x_final = tokenToindex(x, engonehot_dic)\n",
    "y_final = tokenToindex(y, fronehot_dic)\n",
    "for i in range(0, 3):\n",
    "    print(x_final[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bea8d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqLen = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5594b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(data, seqLen):\n",
    "    features = np.zeros((len(data), seqLen),dtype=int)\n",
    "    for i, rev in enumerate(data):\n",
    "        if len(rev) != 0:\n",
    "            features[i, 0] = 1\n",
    "            features[i, 1:len(rev)+1] = np.array(rev)\n",
    "            features[i, len(rev)+1] = 2\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9be7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 746, 8, 628, 2406, 296, 6945, 1144, 1625, 154, 12882, 1528, 2118, 5914, 1427, 3449, 2431, 945, 470, 1984, 945, 943, 171, 229, 12883, 2889]\n",
      "[    1    66   746     8   628  2406   296  6945  1144  1625   154 12882\n",
      "  1528  2118  5914  1427  3449  2431   945   470  1984   945   943   171\n",
      "   229 12883  2889     2     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "x_final_numpy = np.array(x_final, dtype = object)\n",
    "y_final_numpy = np.array(y_final, dtype = object)\n",
    "\n",
    "print(x_final_numpy[-1])\n",
    "x_final_pad = padding(x_final_numpy, seqLen)\n",
    "y_final_pad = padding(y_final_numpy, seqLen)\n",
    "print(x_final_pad[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d03e870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108673\n",
      "108673\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_final_pad, y_final_pad, test_size=0.2, random_state=42)\n",
    "print(len(x_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9d99c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.from_numpy(x_train).to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).to(device)\n",
    "\n",
    "x_test_tensor = torch.from_numpy(x_test).to(device)\n",
    "y_test_tensor = torch.from_numpy(y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87d3d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94de9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "057444d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batchSize)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e483f16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([64, 200])\n",
      "Sample input: \n",
      " tensor([[   1,   11,  622,  ...,    0,    0,    0],\n",
      "        [   1, 2301, 7418,  ...,    0,    0,    0],\n",
      "        [   1,  261,  340,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   1,   62,  792,  ...,    0,    0,    0],\n",
      "        [   1,  238,   22,  ...,    0,    0,    0],\n",
      "        [   1,    2,    0,  ...,    0,    0,    0]], dtype=torch.int32)\n",
      "Sample output: \n",
      " tensor([[   1,   60,   74,  ...,    0,    0,    0],\n",
      "        [   1, 2268,  310,  ...,    0,    0,    0],\n",
      "        [   1, 1951,  111,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   1,  147,  548,  ...,    0,    0,    0],\n",
      "        [   1,    7,   31,  ...,    0,    0,    0],\n",
      "        [   1,    7,    1,  ...,    0,    0,    0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample output: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "594f16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=300) # GloVe is used for english language, this version of GloVe converts the input data into 300 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df97a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDimension = 300\n",
    "hiddenSize = 256\n",
    "numLayer = 2\n",
    "batchFirst = True\n",
    "biDirectional = False\n",
    "direction = 1\n",
    "encoderDropout = 0.3\n",
    "decoderDropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "802e53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, inputDimension, hiddenSize, numLayer, batchFirst, p):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = inputDimension\n",
    "        self.hidden_size = hiddenSize\n",
    "        self.num_layers = numLayer\n",
    "        self.batch_first = batchFirst\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(self.input_size, self.hidden_size, self.num_layers, bidirectional=False,\n",
    "                          batch_first=self.batch_first, dropout=p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.batch_size = input.size(0)\n",
    "        hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(device)\n",
    "        embd = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.lstm(embd, (hidden, cell))\n",
    "        return output, hidden, cell # output contains the entire encoder state, hidden contains the last hidden state of the encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99dac949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanuAttention(torch.nn.Module):\n",
    "    def __init__(self, hiddenSize):\n",
    "        super(BahdanuAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hiddenSize\n",
    "        self.W_a = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.U_a = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.v_a = nn.Linear(self.hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)\n",
    "        \n",
    "        #print(decoder_hidden.shape)\n",
    "        #print(encoder_outputs.shape)\n",
    "        \n",
    "        attention_energy = torch.tanh(self.W_a(decoder_hidden) + self.U_a(encoder_outputs))\n",
    "        attention_scores = self.v_a(attention_energy).squeeze(2)\n",
    "\n",
    "        # Apply softmax to obtain attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Calculate the context vector by weighted sum\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0061b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd2b0501",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'fr'\n",
    "# the dimensionality of the embeddings is determined by the pre-trained FastText model itself.\n",
    "#The dimensionality is typically set during the training of the FastText model and is not a configurable parameter\n",
    "#when loading the embeddings.\n",
    "ft_vectors = FastText(language=language, cache='cache.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41ad77fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesan\\AppData\\Local\\Temp\\ipykernel_10732\\2224561773.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pretrained_embeddings = torch.tensor(ft_vectors.vectors)\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = torch.tensor(ft_vectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf05131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, inputDimension, hiddenSize, numLayer, batchFirst, p, outputSize, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = inputDimension\n",
    "        self.hidden_size = hiddenSize\n",
    "        self.num_layers = numLayer\n",
    "        self.batch_first = batchFirst\n",
    "        self.output_size = outputSize\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(ft_vectors.vectors))\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(self.hidden_size + self.input_size, self.hidden_size, self.num_layers, bidirectional=False,\n",
    "                          batch_first=self.batch_first, dropout=p)\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        # shape of input: (batchSize) but we want it as (batchsize, seqLen) -> (batchsize, 1)\n",
    "        input = input.unsqueeze(1)\n",
    "        #print(f'shape of input: {input.shape}')\n",
    "        embd = self.dropout(self.embedding(input))\n",
    "        \n",
    "        \n",
    "        context_vector, attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embd, context_vector), dim=2)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "\n",
    "        #prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        prediction = self.fc(output)\n",
    "        #print(f'prediction shape: {prediction.shape}')\n",
    "        prediction = prediction.squeeze(1)\n",
    "\n",
    "        return prediction, hidden, cell, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40b421f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDimensionFr = pretrained_embeddings.shape[1] # embedding dimension for french words\n",
    "outputSize = len(fronehot_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "327b6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, sourceLang, targetLang, seqLen, outputSize):\n",
    "        #print(sourceLang.shape)\n",
    "        batch_size = sourceLang.shape[0]\n",
    "        target_len = seqLen\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, outputSize).to(device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(sourceLang)\n",
    "\n",
    "        x = targetLang[:, 0]\n",
    "\n",
    "        for t in range(1, 100):\n",
    "            output, hidden, cell, att = self.decoder(x, hidden, cell, encoder_outputs)\n",
    "            #output = F.softmax(output, dim=1)\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "            x = targetLang[:, t] if random.random() < 0.5 else best_guess\n",
    "            #print(f'shape of best guess: {best_guess.shape}')\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f6b06c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesan\\AppData\\Local\\Temp\\ipykernel_10732\\1945845758.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(ft_vectors.vectors))\n"
     ]
    }
   ],
   "source": [
    "encoderNet = Encoder(inputDimension, hiddenSize, numLayer, batchFirst, encoderDropout).to(device)\n",
    "attention = BahdanuAttention(hiddenSize)\n",
    "decoderNet = Decoder(inputDimensionFr, hiddenSize, numLayer, batchFirst, decoderDropout, outputSize, attention).to(device)\n",
    "modelNet = seq2seq(encoderNet, decoderNet).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ea72758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([200, 64, 21198])\n"
     ]
    }
   ],
   "source": [
    "output = modelNet(sample_x,sample_y, seqLen, outputSize)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56d1c2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 473.772027 million\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in modelNet.parameters())\n",
    "total_params /= 1000000\n",
    "print(f\"Total number of parameters: {total_params} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c94865b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelNet.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5562c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bae31ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epoch: 0\n",
      "9.943492889404297\n",
      "9.822513580322266\n",
      "9.615100860595703\n",
      "9.147562026977539\n",
      "8.38921070098877\n",
      "7.652894020080566\n",
      "7.03782320022583\n",
      "6.544252872467041\n",
      "6.111898422241211\n",
      "5.770042419433594\n",
      "5.543092727661133\n",
      "5.362544059753418\n",
      "5.282080173492432\n",
      "5.217280387878418\n",
      "5.174806594848633\n",
      "5.168555736541748\n",
      "5.146718502044678\n",
      "5.1657843589782715\n",
      "5.148985385894775\n",
      "5.149369239807129\n",
      "5.150292873382568\n",
      "5.144721508026123\n",
      "5.148214340209961\n",
      "5.130126953125\n",
      "5.151516437530518\n",
      "5.159714698791504\n",
      "5.152123928070068\n",
      "5.150298118591309\n",
      "5.150794982910156\n",
      "5.165882587432861\n",
      "5.1405720710754395\n",
      "5.1391072273254395\n",
      "5.1371989250183105\n",
      "5.145280361175537\n",
      "5.146227836608887\n",
      "5.154328346252441\n",
      "5.148754596710205\n",
      "5.130823612213135\n",
      "5.153972148895264\n",
      "5.1361565589904785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source, target \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m modelNet(source, target, seqLen, outputSize)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# outputs shape -> seqLen, batchsize, output size but want it 2 dimensional\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#print(outputs.shape)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m#print(target.shape)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlcuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[33], line 18\u001b[0m, in \u001b[0;36mseq2seq.forward\u001b[1;34m(self, sourceLang, targetLang, seqLen, outputSize)\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m targetLang[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 18\u001b[0m     output, hidden, cell, att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, hidden, cell, encoder_outputs)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#output = F.softmax(output, dim=1)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     outputs[t] \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlcuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[31], line 34\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, input, hidden, cell, encoder_outputs)\u001b[0m\n\u001b[0;32m     30\u001b[0m output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(rnn_input, (hidden, cell))\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#prediction = self.fc_out(output.squeeze(0))\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#print(f'prediction shape: {prediction.shape}')\u001b[39;00m\n\u001b[0;32m     36\u001b[0m prediction \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlcuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlcuda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clip = 1\n",
    "epochs = 2\n",
    "\n",
    "valid_loss_min = np.Inf\n",
    "epoch_tr_loss, epoch_vl_loss = [], []\n",
    "epoch_tr_acc, epoch_vl_acc = [], []\n",
    "\n",
    "checkpoint_dir = 'checkpoints/'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'model_checkpoint.pth')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Number of epoch: {epoch}')\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    modelNet.train()\n",
    "\n",
    "    for source, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = modelNet(source, target, seqLen, outputSize).to(device)\n",
    "        # outputs shape -> seqLen, batchsize, output size but want it 2 dimensional\n",
    "        #print(outputs.shape)\n",
    "        #print(target.shape)\n",
    "\n",
    "        outputs = outputs[1:].reshape(-1, outputs.shape[2])\n",
    "        target = target[:, 1:].reshape(-1)\n",
    "\n",
    "        #print(outputs.shape)\n",
    "        #print(target.shape)\n",
    "\n",
    "        target = target.long()\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(modelNet.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        print(loss.item())\n",
    "\n",
    "    epoch_tr_loss.append(train_loss / len(train_loader))\n",
    "    print(f'Training Loss: {epoch_tr_loss[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbe579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
